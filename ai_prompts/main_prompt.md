I need to act as professional senior data engineer expert on ETL tasks and apache airflow. I have PostgreSQL database collecting data from balloon probes. Each probe has unique device_id and sends telemetry containing temperature, air pressure, height over ocean level, speed of wind and its current position in latitude and longitude based on GPS. Data are collected into PostgreSQL table each second for each probe unless there is a disruption in communication. I need you to write Apache Airflow DAG which will repeatedly calculate minimal, maximal and average for measurements for each 10 minutes interval (i.e. timestamp of measurement rounded to full 10 minutes) and also distance probe traveled through those 10 minutes. The same results should be also calculated for each hour (i.e. timestamp of measurement rounded to full hour). Suggest also scheduling interval and how DAG will assure not to calculate results repeatedly.
Here are some additional facts:
- whole solution should run in docker, give me corresponding docker-compose file for the whole task
- for collecting database latest version of PostgresSQL is used, it runs in docker, give me instruction how to use docker volumes to preserve data over restarts of the container
- structure of PostgreSQL table collecting data is - device_id unique UUID, time as timestamp of measurement in unix format, temperature float in degree celsius, air_pressure float in Pascals, height float in meters, wind_speed float in metres per second, position as JSON structure containing longitude and latitude.
- output data should be written into MySQL database, it also runs in docker, also give me instruction how to use docker volumes for it
- suggest best suitable structure for output MySQL table, you have full liberty to define best suitable table structure
- specify database connection credentials in external secret files for each database, link them to containers, these files will be stored locally in git repository, not committed to github, for testing purposes it is sufficient
- use some generic names for databases and logins, I will later change them manually to necessary values
- compute distance as geodesic distance between 2 points on Earth based on latitude and longitude
- output table should have special flag in row showing that results for specific row (either interval 10 minutes or 60 minutes) have gaps. Gap is defined as any missing measurement record not present in database. Add also column summarizing how many measurements are missing in given interval. So each missing record for 1 second per probe is understood as a gap in data. We expect having gaps, we just need to know how reliable are data in given time period.
- for airflow setup I will use already existing docker-compose file so no need to create this part
- for testing we will process just 3 records per second. Give me estimates of performance degradation in case of huge amounts of data being processed.
Before you give me the solution ask me what else you need to know.